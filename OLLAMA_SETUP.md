# ü§ñ Opci√≥n B: Ollama - IA Local Gratuita

## ¬øQu√© es Ollama?
Ollama te permite correr modelos de IA **100% gratis** en tu servidor local, sin necesidad de APIs externas ni l√≠mites de uso.

---

## üì• Instalaci√≥n de Ollama

### **Paso 1: Descargar Ollama**
Descarga desde: https://ollama.com/download

**Para Windows:**
1. Descarga el instalador `.exe`
2. Ejecuta y sigue el asistente
3. Ollama quedar√° corriendo en segundo plano

**Para Linux/Mac:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

---

## üöÄ Paso 2: Descargar un Modelo

Una vez instalado, abre una terminal y ejecuta:

```bash
# Modelo recomendado para tu caso (r√°pido y preciso)
ollama pull llama3.2:3b

# O si tienes m√°s RAM (mejor calidad)
ollama pull llama3.2:7b

# O para espa√±ol espec√≠fico
ollama pull mistral:7b
```

**Modelos disponibles:**
- `llama3.2:3b` - 2GB RAM, muy r√°pido ‚úÖ **Recomendado**
- `llama3.2:7b` - 4GB RAM, m√°s preciso
- `mistral:7b` - 4GB RAM, excelente para espa√±ol
- `gemma2:2b` - 1.5GB RAM, ultra r√°pido

---

## üîß Paso 3: Verificar que funciona

```bash
ollama run llama3.2:3b
```

Deber√≠as ver un chat interactivo. Escribe `exit` para salir.

---

## üíª Paso 4: Integrar con IslaControl

### **Crear API endpoint en Laravel**

1. **Crea un controlador:**

```bash
cd C:\xampp\htdocs\Islacontrol
php artisan make:controller OllamaController
```

2. **Edita el controlador** (`app/Http/Controllers/OllamaController.php`):

```php
<?php

namespace App\Http\Controllers;

use Illuminate\Http\Request;
use Illuminate\Support\Facades\Http;

class OllamaController extends Controller
{
    public function chat(Request $request)
    {
        $question = $request->input('question');
        $context = $request->input('context', []);

        try {
            $response = Http::timeout(30)->post('http://localhost:11434/api/generate', [
                'model' => 'llama3.2:3b',
                'prompt' => "Eres IslaFinance IA, asistente de negocios. Analiza estos datos: " . json_encode($context) . "\n\nPregunta del usuario: " . $question,
                'stream' => false
            ]);

            return response()->json([
                'success' => true,
                'response' => $response->json()['response']
            ]);

        } catch (\Exception $e) {
            return response()->json([
                'success' => false,
                'error' => $e->getMessage()
            ], 500);
        }
    }
}
```

3. **Agrega la ruta** (`routes/web.php`):

```php
Route::post('/api/ollama/chat', [OllamaController::class, 'chat']);
```

4. **Modifica el JavaScript** en `dashboard.blade.php`:

En la funci√≥n `generateResponse`, agrega al final (antes del return):

```javascript
// Intentar usar Ollama si est√° disponible
try {
    const ollamaResponse = await fetch('/api/ollama/chat', {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'X-CSRF-TOKEN': csrfToken
        },
        body: JSON.stringify({
            question: question,
            context: {
                sales: metrics.totalSales,
                revenue: metrics.revenueThisMonth,
                products: metrics.totalProducts,
                lowStock: metrics.lowStock,
                trend: metrics.salesTrend
            }
        })
    });

    if (ollamaResponse.ok) {
        const data = await ollamaResponse.json();
        if (data.success) {
            return {
                response: data.response,
                confidence: 0.95
            };
        }
    }
} catch (error) {
    console.log('Ollama no disponible, usando IA local');
}

// Si Ollama no est√° disponible, usar respuestas predefinidas
return {
    response,
    confidence: 0.90
};
```

---

## ‚úÖ Verificar que funciona

1. **Aseg√∫rate de que Ollama est√° corriendo:**
```bash
ollama list
```

Deber√≠as ver el modelo descargado.

2. **Prueba el endpoint desde navegador:**
```
http://localhost:11434/
```

Deber√≠as ver "Ollama is running"

3. **Recarga IslaControl y prueba la IA**

---

## üìä Comparaci√≥n de Opciones

| Caracter√≠stica | IA Mejorada (A) | Ollama (B) |
|----------------|-----------------|------------|
| **Costo** | $0 | $0 |
| **Instalaci√≥n** | ‚úÖ Ya funciona | Requiere setup |
| **Velocidad** | ‚ö° Instant√°nea | üê¢ 2-5 segundos |
| **Calidad** | ‚≠ê‚≠ê‚≠ê Buena | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excelente |
| **Contexto** | Limitado | Conversacional |
| **Internet** | No requiere | No requiere |
| **RAM necesaria** | M√≠nima | 2-8 GB |

---

## üéØ Recomendaci√≥n

- **Usa Opci√≥n A (IA Mejorada)** ‚Üí Ya est√° funcionando, r√°pida, sin setup
- **Agrega Opci√≥n B (Ollama)** ‚Üí Si quieres conversaciones m√°s naturales

Puedes tener **AMBAS**:
1. IA Mejorada para respuestas r√°pidas
2. Ollama para an√°lisis complejos (se activa autom√°ticamente si est√° disponible)

---

## üÜò Problemas comunes

**Ollama no inicia:**
```bash
# Windows
net start ollama

# Linux/Mac
sudo systemctl start ollama
```

**Modelo muy lento:**
- Usa `llama3.2:3b` en lugar de modelos m√°s grandes
- Aseg√∫rate de tener suficiente RAM

**Error de conexi√≥n:**
- Verifica que Ollama est√© en `http://localhost:11434`
- Revisa firewall/antivirus

---

## üöÄ Siguiente nivel

Una vez que funcione, puedes:
- Entrenar con tus propios datos
- Usar modelos especializados
- Implementar memoria de conversaciones
- Agregar an√°lisis de im√°genes
